{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i have 3 folders, train test and sample\n",
    "# write code to import training csv and testing csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import the training data\n",
    "train_data = pd.read_csv('train/train.csv')\n",
    "\n",
    "# Import the testing data\n",
    "test_data = pd.read_csv('test/test.csv')\n",
    "\n",
    "\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in train data , \n",
    "from skimpy import skim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = train_data.drop('id', axis = 1)\n",
    "\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "# Extract relevant features from the datetime columns\n",
    "df['pickup_year'] = df['pickup_datetime'].dt.year\n",
    "df['pickup_month'] = df['pickup_datetime'].dt.month\n",
    "df['pickup_day'] = df['pickup_datetime'].dt.day\n",
    "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "df['pickup_minute'] = df['pickup_datetime'].dt.minute\n",
    "\n",
    "# df['dropoff_year'] = df['dropoff_datetime'].dt.year\n",
    "# df['dropoff_month'] = df['dropoff_datetime'].dt.month\n",
    "# df['dropoff_day'] = df['dropoff_datetime'].dt.day\n",
    "# df['dropoff_hour'] = df['dropoff_datetime'].dt.hour\n",
    "# df['dropoff_minute'] = df['dropoff_datetime'].dt.minute\n",
    "\n",
    "df = df.drop(['pickup_year'], axis=1)\n",
    "\n",
    "# Drop the original datetime columns\n",
    "df.drop(['pickup_datetime', 'dropoff_datetime'], axis=1, inplace=True)\n",
    "\n",
    "# Scale the data using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# df[['pickup_year', 'pickup_month', 'pickup_day', 'pickup_hour', 'pickup_minute',\n",
    "#      'dropoff_year', 'dropoff_month', 'dropoff_day', 'dropoff_hour', 'dropoff_minute']] = scaler.fit_transform(df[['pickup_year', 'pickup_month', 'pickup_day', 'pickup_hour', 'pickup_minute',\n",
    "#                                                                                                                  'dropoff_year', 'dropoff_month', 'dropoff_day', 'dropoff_hour', 'dropoff_minute']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in train data , \n",
    "from skimpy import skim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df_test = test_data.drop('id', axis = 1)\n",
    "\n",
    "df_test['pickup_datetime'] = pd.to_datetime(df_test['pickup_datetime'])\n",
    "# df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "# Extract relevant features from the datetime columns\n",
    "df_test['pickup_year'] = df_test['pickup_datetime'].dt.year\n",
    "df_test['pickup_month'] = df_test['pickup_datetime'].dt.month\n",
    "df_test['pickup_day'] = df_test['pickup_datetime'].dt.day\n",
    "df_test['pickup_hour'] = df_test['pickup_datetime'].dt.hour\n",
    "df_test['pickup_minute'] = df_test['pickup_datetime'].dt.minute\n",
    "\n",
    "df_test = df_test.drop(['pickup_year'], axis=1)\n",
    "\n",
    "# Drop the original datetime columns\n",
    "df_test.drop(['pickup_datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to perform cyclical transformation\n",
    "def cyclical_transform(df, col, max_val):\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Apply cyclical transformation to date, month, minute, and hour\n",
    "cyclical_transform(df, 'pickup_day', 31)\n",
    "cyclical_transform(df, 'pickup_month', 12)\n",
    "cyclical_transform(df, 'pickup_minute', 60)\n",
    "cyclical_transform(df, 'pickup_hour', 24)\n",
    "\n",
    "# cyclical_transform(df, 'dropoff_day', 31)\n",
    "# cyclical_transform(df, 'dropoff_month', 12)\n",
    "# cyclical_transform(df, 'dropoff_minute', 60)\n",
    "# cyclical_transform(df, 'dropoff_hour', 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to perform cyclical transformation\n",
    "def cyclical_transform(df, col, max_val):\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Apply cyclical transformation to date, month, minute, and hour\n",
    "cyclical_transform(df_test, 'pickup_day', 31)\n",
    "cyclical_transform(df_test, 'pickup_month', 12)\n",
    "cyclical_transform(df_test, 'pickup_minute', 60)\n",
    "cyclical_transform(df_test, 'pickup_hour', 24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['passenger_count_log'] = np.sqrt(df['passenger_count'] + 1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[['passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','vendor_id']] = scaler.fit_transform(df[['passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','vendor_id']])\n",
    "df = df.drop(['passenger_count_log'],axis = 1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['passenger_count_log'] = np.sqrt(df_test['passenger_count'] + 1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_test[['passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','vendor_id']] = scaler.fit_transform(df_test[['passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','vendor_id']])\n",
    "df_test = df_test.drop(['passenger_count_log'],axis = 1)\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['store_and_fwd_flag'])\n",
    "df_test = pd.get_dummies(df_test, columns=['store_and_fwd_flag'])\n",
    "\n",
    "skim(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 13891572.42\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop('trip_duration', axis=1)\n",
    "y = df['trip_duration']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "\n",
    "# You can also use other metrics such as mean absolute error, R-squared, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store the feature importances\n",
    "feature_importances_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Print the top 10 most important features\n",
    "print(feature_importances_df.head(10))\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the trained Random Forest Regressor model to a file\n",
    "joblib.dump(rf, 'random_forest_regressor.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skim(df)\n",
    "\n",
    "print(X_test.columns)\n",
    "print(df_test.columns)\n",
    "\n",
    "\n",
    "y_test_pred = rf.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this line if you want to try Gradient Boosting Regressor\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# # Create a Gradient Boosting Regressor model\n",
    "# gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# # Train the model on the training data\n",
    "# gbr.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# y_pred = gbr.predict(X_test)\n",
    "\n",
    "# # Evaluate the model using mean squared error\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f'Mean Squared Error: {mse:.2f}')\n",
    "\n",
    "# # Get the feature importances\n",
    "# feature_importances = gbr.feature_importances_\n",
    "\n",
    "# # Create a DataFrame to store the feature importances\n",
    "# feature_importances_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "\n",
    "# # Sort the DataFrame by importance in descending order\n",
    "# feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# # Print the top 10 most important features\n",
    "# print(feature_importances_df.head(10))\n",
    "\n",
    "# # Create a DataFrame with the predictions\n",
    "# predictions_df = pd.DataFrame({'trip_duration': y_pred})\n",
    "\n",
    "# # Save the predictions to a CSV file\n",
    "# predictions_df.to_csv('trip_duration_predictions_gbr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the predictions\n",
    "predictions_df = pd.DataFrame({'trip_duration': y_test_pred})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('trip_duration_predictions_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./test/test - Copy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
